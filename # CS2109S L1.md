# CS2109S L1
##  What is ai
Intelligent Agents: A rational agent wil choose the action which maximize the outcome
Agent   <-percepts  
Sensors				Environment
Function
Actuators  ->actions
## Properties of Task Environment
* Fully obversable vs Partially obversable
* Deterministic vs Stochastic
	The next state of the environment is completely determined by the current state and the action executed by the agent/ The environment is also dependent on the action of other agents, then it is also strategic (unless the other agents are predictable)
* Episodic vs Sequential
	The agent's experience is divided into atoimc "episodes" ,and the choice of action in each episode depends onku on the episode itself./Sequential means the previous action affects now situation.
* Static/Dynamic
	The environment is unchanged while agent is deliberating
	The environment is semi-dynamic if the environment itself does not change with the passage with the passage of time, but the agent's performance scores does.
* Discrete vs Continuous
	A limited number of distinct, clearly defined percepts and actions.
* Single agent/ multi agent
## The agent function
The agent function maps from percept histories to action
* Simple reflex agent(reflex simply based on the situation)
* Goal-based Agent
* Utility-based Agent
* Learning Agent
## Exploration vs Exploitation
## How to build an agent
### Search problems
The goal is to find a state  from a set of possible states by exploring various possibilities
To solve such problems ,we need
* A goal
* A model of the environment
* A search algorythm   
---
# L2 Search
## 1.Search Terms:
Search Tree
Path Cost:The cost of a paath from any state to any state
OPtimal path cost : the cost of the lowest-cost from any state to any state
## Evaluation criteria
1. Complexity:
Time Complexity: number of nodes generated or expanded
Spaace Complexity: max number of node in memory

2. Completeness and Optimality:
Complete:An algoryth, is complete if for every problem instance . it will find a solution if it exist
Optimal : If produce a solution, the solution is the best possible
## Uninformed search
* BFS:use a queue to maintain the frontier
	Time complexity: exponential
	Space: exponential
	Complete: exponential
	Optimal: If all cost is the same
* Uniform-cost search(UCS) : use a priority queue
	Time : exponential(tier)
	Space: exponential(size of tier)
	Complete: yes
	Optimal: yes
* DFS :
	Time: Exponential
	Space: Polynomial
	Complete: NO, when the depth is infinite(e.g. reversible)
	Optimal: No, may exist more shallow solution
## Heuristic
A heuristic is an estimate of the optimal path cost from ant state to the goal state

* $A *  search: f(n) = g(n) + h(n)
  g is the cost to reach n, h is the heuristic from n to goal
	Time: exponential
	Space(frontier): exponential
    Complete: Yes if edge cost is positive and branch in finite
    Optimal: Depends on the heuristic
* Admissible Heuristic
* A heurisitc h(n) is admissible if fir every node n, h(n) <= h * (n) , which is the optimal path cost to reach the goal state from n
* Theorem: A * search is optimal with admissible heuristic without visited memory is optimal
* Consistent Heurisitc: for every node n . every successor n' of n generated by any action a, h(n)<=c(n,a,n') + h(n'), and h(G) = 0.
* Theorem :A * search is optimal with consistent heuristic with visited memory is optimal
* Dominance: if h1(n) >= h2(n),then h1 dominates h2, h1 is better for search if admissible
## Search strategy:
* Depth limited Search(DLS): Limit te=he search to depth L, Backtrack when the limit is hit
Time : exponential
Space: polynomial
Complete: No
Optimal: No
* Iterative Deepening Search: Search with depth limit 0,.....
Time : exponential
Space: polynomial
Complete: Yes
Optimal: Yes
*
---
# L3
## 1. Systematic/Local Search
Local Search:
* Only locallu reachable states
* typically incomplete and suboptimal
* Anytime property: longer runtime, better solution

Problem formulation in local search
* State : Represent different configurations of candidate solutions, may or may not map to an actual problem state, but are used to represenet potential solutions
* Initial State: Starting configuration
* Goal Test: CHeck if it is the desired solution
* Successor function:Generate neighbor states by applying modifications
*

Evaluation function:
A mathematical function used to access the quality or desirability of a state
e.g. in n queens problem, can be the number of safe queens

State Space Landscape
* Global maximum: The overal hifhest point or sollution across the entire state sapce that represents optimal solution
* Local Maximum: A point tn the state space that is higher than its immediate nerghbors, nut there may be higher values globally
*

## 2. Adversarial Search
Adverarial games: one player's gain is the other player's loss

Problem Formulatuon:
States
INitial State
Terminal States:: state where the game ends
Actions
Transition
Utility Functiobn:
output the value of a staet from the perspective of our agent

Minimax
Algorithm fro two-plaer zero-sum game
core assumption: all players play optimally
expand function : expland(state)
for each state , compute next_state = transition(state, action)
Terminal function: return is is terminal
untility function: return the score of the agent

Theorem : In any finite , two-player , zero sum game of perfect information, the minimax algorith, comoputes the optimal strategy for both playerz , guaranteeing th ebest achievable outcome ofr each, assuming both play optimally

Theorem: If PLayer A' opponent deviaates from optimal play (plays sub-optimally), then for any action taken by player A, the utility obtained by player A is never less than the utility they would obtain against an optimal opponent

Alpha-Beta pruning:
Maintain the best outcomes so far for player A and Player B:
	Highest value for A for player A, lowest value for A for player B

# L4 Regression

## 1. Background
* Vector
	* Column Vector : w
	* Row Vector : $w^T$
* Dot Product
* Linear Model
	$h_w(x) = w_0x_0+....+w_dx_d    w_0 = 1$ is a dummy variable
    $h_w(x) = w^Tx$
* Loss
	$ J_{MSE}(w) = \frac{1}{2n}\sum_{i=1}^n (h_w(x^{(i)}) - y^{(i)})^2 $
* Gradient and partial derivative:
	for $h_w(x) = w_0x_0+....+w_dx_d$
	The gradient = $[x1,x2,x3,...xd]^T$
* Minimizing a function:
	The minimun of a function is when the gradient = 0
* finding best linear model:
## 2. Learning with normal equations
## 3. Learning with gradient
* Gradient descent:
	* Start with some w
	* Update w with a step in the opposite direction of the gradient
	* $w_j <-- w_j - \gamma \frac{\partial J(w_0,w_1,...)}{\partial w_j}$
	* The Learining rate$\gamma>0$ is a hyperparameter that determines the step size
	* Repeat until termination criterion
	* Can use many parameters

* Convexity
	* Consider a real-valued one-dimensional function and any line segment connecting any two distinct points on the function's graph, this function is called 
	* Convex if the line is above or on the graph
	* Strictly convex if the line is always above the grap
	* For convex , the local minimum is also global minimum
	* For strictly convex, the function has at most one unique global minimum

	* Theorem : MSE loss function for a linear regression model is a convex function with respect to the model's parameters
	* Definition: Linearly dependent features. A feature is linearlu dependent on other features, if it is a linear combination of the other features.
	* Theorem: If the features in the training data are linearly independent, the function(MSE + linear model) is strictly convex.
	* Theorem: For  a linear regression model with a MeanSquared Error(MSE) loss function, the Gradient Descent algorith, is guaranteed tp converge to a global minimum, provided the learning rate is chosen appopriately. If the features are linearly independent, this global minimum is unique

###  A problem with gradient descent:
	Feature may have different scales
* min-max scaling
	$x_i' = \frac{x_i-min(x_i)}{max(x_i)-min(x_i)}$
	Scales the features to be within [0,1]
* Standardization
	$x_i' = \frac{x_i-\mu_i}{\delta_i}$
* Different solution: Differernt learning rate
### Other problems with gradient descent
* On large datasets, gradient descent will be very slow because it needs to consider the entire dataset to compute the gradient
* Gradient descent may get stuck at a local minimum or plateau on non-convex optimization.

* Variants of Gradient Descent
	* (Batch) Gradient Descent
	* Mini-batch Gradient escent
		Consider a subset of training examples at a time
		Cheaper per iteration
	* Stochastic Gradient Descent
		Select one random traning example at a time
## 4. Feature Transformation
* We can take a d-dimensional feature vector and transform it into a M-dimensional feature vector. Usually M>=d
	$x\in R^d -> \phi(x) \in R^M $
	* The function $\phi$ is called a feature transformation or feature map,
	* The vector $\phi(x)$ is called transformed features.

* Example:
	* Linear: $\phi(x) -> x$
	* Polynomial features of degree k, $\phi_{pk}(x)$
		$\phi_{p2}([x_1,x_2]) -> [x_1,x_2,x_1x_2,x_1^2,x_2^2]$
	* Log: For each $x_i$, transform $x_i->x_i and log(x_i)$
	* Exponential: For each $x_i$, transform $x_i->x_i and exp(x_i)$

* A General View of ML Models
	* A Machine learning model $\hat{h} is usually of the following form$
		$\hat{h}(x) = h(\phi(x))$
	* $\phi$ is called the feature transformation/mapping/extraction function
		It may do nothing,maybe handcraft, may be learned
	* h is called the predictor
		It is usually a simple function. It is the one performing prediction





