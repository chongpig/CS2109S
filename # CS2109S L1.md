# CS2109S L1
##  What is ai
Intelligent Agents: A rational agent wil choose the action which maximize the outcome
Agent   <-percepts  
Sensors				Environment
Function
Actuators  ->actions
## Properties of Task Environment
* Fully obversable vs Partially obversable
* Deterministic vs Stochastic
	The next state of the environment is completely determined by the current state and the action executed by the agent/ The environment is also dependent on the action of other agents, then it is also strategic (unless the other agents are predictable)
* Episodic vs Sequential
	The agent's experience is divided into atoimc "episodes" ,and the choice of action in each episode depends onku on the episode itself./Sequential means the previous action affects now situation.
* Static/Dynamic
	The environment is unchanged while agent is deliberating
	The environment is semi-dynamic if the environment itself does not change with the passage with the passage of time, but the agent's performance scores does.
* Discrete vs Continuous
	A limited number of distinct, clearly defined percepts and actions.
* Single agent/ multi agent
## The agent function
The agent function maps from percept histories to action
* Simple reflex agent(reflex simply based on the situation)
* Goal-based Agent
* Utility-based Agent
* Learning Agent
## Exploration vs Exploitation
## How to build an agent
### Search problems
The goal is to find a state  from a set of possible states by exploring various possibilities
To solve such problems ,we need
* A goal
* A model of the environment
* A search algorythm   
---
# L2 Search
## 1.Search Terms:
Search Tree
Path Cost:The cost of a paath from any state to any state
OPtimal path cost : the cost of the lowest-cost from any state to any state
## Evaluation criteria
1. Complexity:
Time Complexity: number of nodes generated or expanded
Spaace Complexity: max number of node in memory

2. Completeness and Optimality:
Complete:An algoryth, is complete if for every problem instance . it will find a solution if it exist
Optimal : If produce a solution, the solution is the best possible
## Uninformed search
* BFS:use a queue to maintain the frontier
	Time complexity: exponential
	Space: exponential
	Complete: exponential
	Optimal: If all cost is the same
* Uniform-cost search(UCS) : use a priority queue
	Time : exponential(tier)
	Space: exponential(size of tier)
	Complete: yes
	Optimal: yes
* DFS :
	Time: Exponential
	Space: Polynomial
	Complete: NO, when the depth is infinite(e.g. reversible)
	Optimal: No, may exist more shallow solution
## Heuristic
A heuristic is an estimate of the optimal path cost from ant state to the goal state

* $A *  search: f(n) = g(n) + h(n)
  g is the cost to reach n, h is the heuristic from n to goal
	Time: exponential
	Space(frontier): exponential
    Complete: Yes if edge cost is positive and branch in finite
    Optimal: Depends on the heuristic
* Admissible Heuristic
* A heurisitc h(n) is admissible if fir every node n, h(n) <= h * (n) , which is the optimal path cost to reach the goal state from n
* Theorem: A * search is optimal with admissible heuristic without visited memory is optimal
* Consistent Heurisitc: for every node n . every successor n' of n generated by any action a, h(n)<=c(n,a,n') + h(n'), and h(G) = 0.
* Theorem :A * search is optimal with consistent heuristic with visited memory is optimal
* Dominance: if h1(n) >= h2(n),then h1 dominates h2, h1 is better for search if admissible
## Search strategy:
* Depth limited Search(DLS): Limit te=he search to depth L, Backtrack when the limit is hit
Time : exponential
Space: polynomial
Complete: No
Optimal: No
* Iterative Deepening Search: Search with depth limit 0,.....
Time : exponential
Space: polynomial
Complete: Yes
Optimal: Yes
*
---
# L3
## 1. Systematic/Local Search
Local Search:
* Only locallu reachable states
* typically incomplete and suboptimal
* Anytime property: longer runtime, better solution

Problem formulation in local search
* State : Represent different configurations of candidate solutions, may or may not map to an actual problem state, but are used to represenet potential solutions
* Initial State: Starting configuration
* Goal Test: CHeck if it is the desired solution
* Successor function:Generate neighbor states by applying modifications
*

Evaluation function:
A mathematical function used to access the quality or desirability of a state
e.g. in n queens problem, can be the number of safe queens

State Space Landscape
* Global maximum: The overal hifhest point or sollution across the entire state sapce that represents optimal solution
* Local Maximum: A point tn the state space that is higher than its immediate nerghbors, nut there may be higher values globally
*

## 2. Adversarial Search
Adverarial games: one player's gain is the other player's loss

Problem Formulatuon:
States
INitial State
Terminal States:: state where the game ends
Actions
Transition
Utility Functiobn:
output the value of a staet from the perspective of our agent

Minimax
Algorithm fro two-plaer zero-sum game
core assumption: all players play optimally
expand function : expland(state)
for each state , compute next_state = transition(state, action)
Terminal function: return is is terminal
untility function: return the score of the agent

Theorem : In any finite , two-player , zero sum game of perfect information, the minimax algorith, comoputes the optimal strategy for both playerz , guaranteeing th ebest achievable outcome ofr each, assuming both play optimally

Theorem: If PLayer A' opponent deviaates from optimal play (plays sub-optimally), then for any action taken by player A, the utility obtained by player A is never less than the utility they would obtain against an optimal opponent

Alpha-Beta pruning:
Maintain the best outcomes so far for player A and Player B:
	Highest value for A for player A, lowest value for A for player B

# L4 
## 1. Machine learning
### 1.1 Problem
* Some problem are inherently intractable to solve meaning that no efficient solution exists for all cases
	e.g. game of go, Protein folding
* Other problems are difficult to solve because formulationg the rules in a way that a computer can comprehend and process is challenging.
	e.g. recognizing numbers in an images, answering an open end question
	
* A new paradigm: learning agent:
	Rather than solvin gthe problem expliciythrough search or by applying a set of rules, Construct an agent that learns a function to identify patterns in the data and make decisions or provide solutions, based on what the agent has learned
	Designing aan agent for [roblems where the function os difficult to specify, or solutions are intractable to compute 
### 1.2 Machine learning
* A subfield of AI that gives computers the ability to learn without being ecplicity prorgrammed
* Types
	1. Supervised Learning:	Learn from labeled data tp learn a mapping from inputs to outputs
	2. Unsupervised Learning : Learn from unlabeled data ro find patterns of structure
### 1.3 Supervised Learning
Probelm-solving using supervised learning involves the following components:
Data: A set of input-output pairs that the model learns from
Model: A funfction that predicts the output based on the input features
Loss: A function that is often used by a learning algorithhm to assess the model's ability in making predictions over a set of data

A learning algorithm is an algorithm which seeks to minimize the loss of a model over a dataset, called the training dataset
	It is used during the training phase of the model anda results ina trained model	
* After the training phase , the model is ready to be used
	However, before the trained model is used in the wild , it''s usually tested on a set of unseen data, called test set.
	This phase is known as the testing/evaluation phase
### 1.4 Tasks
* Classification: 
	Objective: predict a discrete label/class/category based on input features
	The output variable is a categoricalvalue
* Regression
	Objective: predict a continuouos numerical value based on input features
	The output variable is a real number
### 1.5 Data
Dataset in supervised leaning is represrnted as as a set of pairs(xi,yi), where
* $x^{(i)} \in R^d$is the input vector of d length/feature of the i-th data point
	In generral, xi can be multi-dimentional array of features
* $y^{(i)}$ is the label for the i-th data point
	$y^{(i)} \in R$ for regression
	$y^{(i)} \in {1,2,...K}$ for classfication with K classes
### 1.6 Model
* A model or hypothesis refers to a function that maps from inputs to outputs h: X-> Y and that can ve learned by a learning algorithm
* We are interested in finding a model h(x) that best predicts the outputs of given inputs in a dataset
### 1.7 Performance measure & Loss functions
* Functions that take in a dataset D = {(x1,y1)....(xn,yn)} and a model h and return a number representing how well h predicts the maping xi -> yi
* Performance measure is a metric that evaluates how well the model performs on a task. It is used during evaluation/ testing
* Loss function is a function that quantifies the difference vetween the model's predictions and the true outputs yi . It is used to guide learning

#### 1.7.1 Performance meaasure- regression
Mean squareed error(MSE)  
	$MSE = \frac{1}{N}\sum_{i=1}^N(\hat{y}^{(i)}-y^{(i)})^2$
Mean absolute error(MAE)
	$MAE = \frac{1}{N}\sum_{i=1}^N|\hat{y}^{(i)}-y^{(i)}|$

Here $h(x^{(i)})=\hat{y}^{(i)}$
* MSE and MAE can also be used as loss function

#### 1.7.2 Performance meaasure-Classification
Accuracy:
	$Accuracy = \frac{1}{N}\sum_{i=1}^N1_{\hat{y}^{(i)}=y^{(i)}}$
Precision: P = TP/(TP+FP) If false positive are costly
Recall: R = TP/(TP + FN) If false negatives are costly
F1 Score: $F1 = \frac{2}{\frac{1}{P}+ \frac{1}{R}}$
### 1.8 Learning Algorithms
* takes in a traingng  set D and seeks to ofind a model h that approximates the true relationship between inoyts and outputs
* Most algorithms utilize a loss funtion to help guide the finding/optimization of the model
* A learning algorithm is usually quite general and can wowrk for different data, model and loss

## 2. Decision Tree
### 2.1 Decision Tree model
* Internal nodes represent tests on features
* Branches correspond to outcomes of the tests
* Leave nodes assign predictions
* A decision is reached by sequentially checking the attibute value at each node, starting form the root and following rhe branches corresponding to the outcomes of these tests, until a leaf node is encountered.
### 2.2 Optimal Decision Tree model
* There generally exist multiple distinct decision tree models that are all consistent with the training data-that is, htey all classify every training example correctly.
* Prefer a "compact" decision tree: the decision tree has the smallest possible numver of nodes. Also called the optimal decision tree.
* A compact decision tree is less likely to be consistent with training example due to chance

* Theorem : The problem of finding an optimal decision tree that is consistent with all the training examples is NP-Hard
* Implication: make tractaable by greedy heristics:
	Make locally optimal choices at each step by selecting the feature that yields the greatest immediate information gain, to partition the data as decisively as possible.
	
* Background: Entropy
	Entropy quantifies the uncertainty or randomness of a random variable
	Let Y be a random variable with outcomes value y1,...yk and probabilities P(y1)....P(yk). Then the entropy is definded as:
	H(Y) = H(P(y1),....,P(yk)) = =$-\sum_{i=i}^kP(y_i)log_2P(y_i)$
	If the outcomes are dicisions, entropy measures the level of uncertainty (or indecisiveness) about a decision.
* Specific-conditional entropy H(Y|X = x) is the entropy of a random variable Y conditioned on the discrete rando, variable X taking a certain fixed value x.
* Conditional entropy H(Y|X) quantifies the amount of uncertainty(or entropy) that remanis about a random variable Y when another random variable X is known.
	$H(Y|X) = \sum_{x \in X}P(X=x)H(Y|X=x)$
	
* Information gain: is a measure of the reduction on uncertainty or indecisiveness about a particular decision after knowing the value of a specififc variable, The formular for informaation gain is: 	IG(Y;X) = H(Y) -H(Y|X)

### 2.3 Decision Tree Learning 
1. Features
* Top-Down
* Greedy
* Recursive
2. Functions
	* choose_attribute(attributes, rows)
		* Accepts a set of attributes (table columns) in the data and the table rows containint the data points.
		* compute the information gain of each attribute in attributes.
		* Returns the attribute that has the highest information gain.
	* majority_decision(rows)
		* Accepts the table rows containing the data points[
		* Returns the most common decision in the data points
3. base cases
	a. rows is empty: return defalut
    b. rows have the same classification:
    	return classification
   	c. attributes is empty:
  		return majority_decision(rows)
### 2.4 Decision Tree- Analysis
Theorem: For any finite set of consistent examples with discrete(categorical) features and a finite set of label classes, there exists a decision tree that is consistent with the examples.
	may cause overfitting(overly amd unnecessarily) complex as it tries to capture unnecessary information in the training data.
### 2.5 Pruning 
* Prunning refers to thr process of removing parts of the tree. The goal is to produce a simpler, more robust tree that generalizes better to unseen data by limiting the representational capability of a decision tree.
* Common pre-pruning method
	* Max depth: limits the maximum depthof the decision tree.
	* Min-sample leaves: set a minimumm threshold for the number if samples required to be in a leaf node
# L5 Regression

## 1. Background
* Vector
	* Column Vector : w
	* Row Vector : $w^T$
* Dot Product
* Linear Model
	$h_w(x) = w_0x_0+....+w_dx_d    w_0 = 1$ is a dummy variable
    $h_w(x) = w^Tx$
* Loss
	$ J_{MSE}(w) = \frac{1}{2n}\sum_{i=1}^n (h_w(x^{(i)}) - y^{(i)})^2 $
* Gradient and partial derivative:
	for $h_w(x) = w_0x_0+....+w_dx_d$
	The gradient = $[x1,x2,x3,...xd]^T$
* Minimizing a function:
	The minimun of a function is when the gradient = 0
* finding best linear model:
## 2. Learning with normal equations
## 3. Learning with gradient
* Gradient descent:
	* Start with some w
	* Update w with a step in the opposite direction of the gradient
	* $w_j <-- w_j - \gamma \frac{\partial J(w_0,w_1,...)}{\partial w_j}$
	* The Learining rate$\gamma>0$ is a hyperparameter that determines the step size
	* Repeat until termination criterion
	* Can use many parameters

* Convexity
	* Consider a real-valued one-dimensional function and any line segment connecting any two distinct points on the function's graph, this function is called 
	* Convex if the line is above or on the graph
	* Strictly convex if the line is always above the grap
	* For convex , the local minimum is also global minimum
	* For strictly convex, the function has at most one unique global minimum

	* Theorem : MSE loss function for a linear regression model is a convex function with respect to the model's parameters
	* Definition: Linearly dependent features. A feature is linearlu dependent on other features, if it is a linear combination of the other features.
	* Theorem: If the features in the training data are linearly independent, the function(MSE + linear model) is strictly convex.
	* Theorem: For  a linear regression model with a MeanSquared Error(MSE) loss function, the Gradient Descent algorith, is guaranteed tp converge to a global minimum, provided the learning rate is chosen appopriately. If the features are linearly independent, this global minimum is unique

###  A problem with gradient descent:
	Feature may have different scales
* min-max scaling
	$x_i' = \frac{x_i-min(x_i)}{max(x_i)-min(x_i)}$
	Scales the features to be within [0,1]
* Standardization
	$x_i' = \frac{x_i-\mu_i}{\delta_i}$
* Different solution: Differernt learning rate
### Other problems with gradient descent
* On large datasets, gradient descent will be very slow because it needs to consider the entire dataset to compute the gradient
* Gradient descent may get stuck at a local minimum or plateau on non-convex optimization.

* Variants of Gradient Descent
	* (Batch) Gradient Descent
	* Mini-batch Gradient escent
		Consider a subset of training examples at a time
		Cheaper per iteration
	* Stochastic Gradient Descent
		Select one random traning example at a time
## 4. Feature Transformation
* We can take a d-dimensional feature vector and transform it into a M-dimensional feature vector. Usually M>=d
	$x\in R^d -> \phi(x) \in R^M $
	* The function $\phi$ is called a feature transformation or feature map,
	* The vector $\phi(x)$ is called transformed features.

* Example:
	* Linear: $\phi(x) -> x$
	* Polynomial features of degree k, $\phi_{pk}(x)$
		$\phi_{p2}([x_1,x_2]) -> [x_1,x_2,x_1x_2,x_1^2,x_2^2]$
	* Log: For each $x_i$, transform $x_i->x_i and log(x_i)$
	* Exponential: For each $x_i$, transform $x_i->x_i and exp(x_i)$

* A General View of ML Models
	* A Machine learning model $\hat{h} is usually of the following form$
		$\hat{h}(x) = h(\phi(x))$
	* $\phi$ is called the feature transformation/mapping/extraction function
		It may do nothing,maybe handcraft, may be learned
	* h is called the predictor
		It is usually a simple function. It is the one performing prediction





